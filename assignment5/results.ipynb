{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP5 - Naive Bayes and & Logistic Regression : \n",
    "---\n",
    "_Author: CHRISTOFOROU Anthony_\\\n",
    "_Due Date: XX-XX-2023_\\\n",
    "_Updated: 29-11-2023_\\\n",
    "_Description: TP4 - AI_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Modules\n",
    "from assignment5.models.classifiers.naive_bayes import NaiveBayesClassifier\n",
    "from assignment5.models.classifiers.logistic_regression import LogisticRegressionClassifier\n",
    "\n",
    "\n",
    "# make figures appear inline\n",
    "matplotlib.rcParams['figure.figsize'] = (15, 8)\n",
    "%matplotlib inline\n",
    "\n",
    "# notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Before starting, we have to import the data and prepare it for the training!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1. Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "train_data_path = 'data/data_train.csv'\n",
    "test_data_path = 'data/data_test.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_data_path)\n",
    "test_df = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>19000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Female</td>\n",
       "      <td>26</td>\n",
       "      <td>43000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Female</td>\n",
       "      <td>27</td>\n",
       "      <td>57000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>76000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender  Age  EstimatedSalary  Purchased\n",
       "0    Male   19            19000          0\n",
       "1    Male   35            20000          0\n",
       "2  Female   26            43000          0\n",
       "3  Female   27            57000          0\n",
       "4    Male   19            76000          0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train data\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Female</td>\n",
       "      <td>53</td>\n",
       "      <td>104000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>75000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Female</td>\n",
       "      <td>38</td>\n",
       "      <td>65000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Female</td>\n",
       "      <td>47</td>\n",
       "      <td>51000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>47</td>\n",
       "      <td>105000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender  Age  EstimatedSalary  Purchased\n",
       "0  Female   53           104000          1\n",
       "1    Male   35            75000          0\n",
       "2  Female   38            65000          0\n",
       "3  Female   47            51000          1\n",
       "4    Male   47           105000          1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test data\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label encoder instance\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Convert 'Gender' to binary variables in both train and test datasets\n",
    "train_df['Gender'] = label_encoder.fit_transform(train_df['Gender'])\n",
    "test_df['Gender'] = label_encoder.transform(test_df['Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>19000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>43000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>57000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>76000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender  Age  EstimatedSalary  Purchased\n",
       "0       1   19            19000          0\n",
       "1       1   35            20000          0\n",
       "2       0   26            43000          0\n",
       "3       0   27            57000          0\n",
       "4       1   19            76000          0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>104000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>75000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>65000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>51000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>105000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender  Age  EstimatedSalary  Purchased\n",
       "0       0   53           104000          1\n",
       "1       1   35            75000          0\n",
       "2       0   38            65000          0\n",
       "3       0   47            51000          1\n",
       "4       1   47           105000          1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Empirical Distribution of the Labels\n",
    "\n",
    "In the `fit` method of the [`NaiveBayesClassifier`](assignment5/models/classifiers/naive_bayes.py) class, the empirical distribution of the labels is accurately calculated. This involves determining the prior probability for each class (label) in the training dataset, a crucial step in understanding the overall distribution of the classes. The prior probability, representing the initial belief about the distribution before considering the features, is computed as follows:\n",
    "\n",
    "```python\n",
    "self.params[c]['prior'] = X_c.shape[0] / X.shape[0]\n",
    "```\n",
    "\n",
    "This line calculates the proportion of instances in the training set that belong to each class \\( c \\), forming the basis for subsequent probability calculations.\n",
    "\n",
    "### 1.2 Estimation of the Parameters of the Covariates' Distributions for Each Label Value\n",
    "\n",
    "Continuing in the `fit` method, the class-specific parameters (`mean` and `variance`) for the Gaussian distribution of each feature are meticulously estimated. This step is fundamental in the Naive Bayes algorithm as it assumes conditional independence of features within each class:\n",
    "\n",
    "```python\n",
    "self.params[c] = {\n",
    "    'mean': X_c.mean(axis=0),\n",
    "    'var': X_c.var(axis=0),\n",
    "    # Additional parameters...\n",
    "}\n",
    "```\n",
    "\n",
    "Here, `X_c.mean(axis=0)` and `X_c.var(axis=0)` compute the mean and variance for each feature in subsets of the data corresponding to class \\( c \\), essential for modeling the feature distribution.\n",
    "\n",
    "### 1.3 Implementation of the Gaussian Density Function\n",
    "\n",
    "The Gaussian density function is adeptly implemented in the `gaussian_density` method. This function is responsible for calculating the likelihood of observing a specific feature value for a given class, adhering to the Gaussian (normal) distribution. The employed formula encapsulates the essence of the Gaussian distribution:\n",
    "\n",
    "```python\n",
    "numerator = np.exp(- (x - mean) ** 2 / (2 * var))\n",
    "denominator = np.sqrt(2 * np.pi * var)\n",
    "return numerator / denominator\n",
    "```\n",
    "\n",
    "In this expression, `x` is the feature value, while `mean` and `var` are the Gaussian parameters for the feature under a specific class.\n",
    "\n",
    "### 1.4 Prediction of Labels Given New Covariates\n",
    "\n",
    "The `predict` method in [`NaiveBayesClassifier`](assignment5/models/classifiers/naive_bayes.py) is ingeniously designed to handle the prediction of labels for new data points. This method adeptly computes the posterior probability for each class based on the new instance's feature values and the parameters estimated from the training data. It then judiciously selects the class with the highest posterior probability as the prediction:\n",
    "\n",
    "```python\n",
    "for x in X:\n",
    "    posteriors = []\n",
    "    for c in self.classes:\n",
    "        prior = np.log(self.params[c]['prior'])\n",
    "        class_conditional = np.sum(np.log(self.gaussian_density(c, x)))\n",
    "        posterior = prior + class_conditional\n",
    "        posteriors.append(posterior)\n",
    "    preds.append(np.argmax(posteriors))\n",
    "```\n",
    "\n",
    "In this procedure, the class with the maximum log-posterior probability, derived from the product of the prior and the likelihoods of the features, is chosen, showcasing the effectiveness of the Naive Bayes algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier Summary:\n",
      "------------------------------------------------------------\n",
      "Class      | Prior      | Mean                 | Variance\n",
      "------------------------------------------------------------\n",
      "0          | 0.7000     | 0.51, 32.27, 60100.84 | 0.25, 64.32, 625023444.67\n",
      "1          | 0.3000     | 0.45, 45.00, 96549.02 | 0.25, 79.67, 1617855440.22\n"
     ]
    }
   ],
   "source": [
    "X_train = train_df.drop('Purchased', axis=1).values\n",
    "y_train = train_df['Purchased'].values\n",
    "\n",
    "nb_classifier = NaiveBayesClassifier()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "print(nb_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Derivations\n",
    "\n",
    "#### (a) $ p(y_i | x_i; w, b) $\n",
    "\n",
    "Given that $ y_i $ follows a Bernoulli distribution and $ p_i = \\sigma(w^T \\cdot x_i + b) $, where $ \\sigma(z) = \\frac{1}{1 + e^{-z}} $, the probability $ p(y_i | x_i; w, b) $ is:\n",
    "\n",
    "$$\n",
    "p(y_i | x_i; w, b) = p_i^{y_i} \\cdot (1 - p_i)^{(1-y_i)}\n",
    "$$\n",
    "\n",
    ">This is the probability of observing the outcome $ y_i $, which can be either 0 or 1, for the given input $ x_i $. The probability $ p_i $ is obtained by passing the linear combination of inputs and parameters $ w \\cdot x_i + b $ through the sigmoid function $ \\sigma $.\n",
    "\n",
    "#### (b) $ \\log(p(y_i | x_i; w, b)) $\n",
    "\n",
    "The log likelihood of $ p(y_i | x_i; w, b) $ is:\n",
    "\n",
    "$$\n",
    "\\log(p(y_i | x_i; w, b)) = y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i)\n",
    "$$\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\"> Taking the logarithm of the probability expression is common practice because it turns products into sums, which are easier to differentiate and numerically more stable to compute. </div>\n",
    "\n",
    "#### (c) $ \\frac{\\partial \\sigma(z)}{\\partial z} $\n",
    "\n",
    "The derivative of the sigmoid function $ \\sigma(z) $ with respect to $ z $ is:\n",
    "\n",
    "$$\n",
    "\\begin{align} \\frac{d\\sigma(z)}{dz} \n",
    "&= \\frac{d}{dz} \\frac{1}{1 + \\exp(-z)} \\\\ \n",
    "&= \\frac{d}{dz} (1 + \\exp(-z))^{-1} \\\\ \n",
    "\n",
    "&= -(1 + \\exp(-z))^{-2} \\cdot \\frac{d}{dz} (1 + \\exp(-z)) \\\\ \n",
    "&= -(1 + \\exp(-z))^{-2} \\cdot (-\\exp(-z)) \\\\\n",
    "&= \\frac{1}{1 + \\exp(-z)} \\cdot \\frac{\\exp(-z)}{1 + \\exp(-z)} \\\\ \n",
    "&= \\sigma(z) \\cdot \\frac{\\exp(-z)}{1 + \\exp(-z)} \\\\ \n",
    "&= \\sigma(z) \\cdot \\left(1 - \\frac{1}{1 + \\exp(-z)}\\right) \\\\ \n",
    "&= \\sigma(z) \\cdot \\left(1 - \\sigma(z)\\right) \\end{align}\n",
    "$$\n",
    "\n",
    "> This derivative is a key component in the gradient descent algorithm used to update the weights of the logistic regression model. It indicates how a change in the weighted sum $ z $ affects the probability $ \\sigma(z) $.\n",
    "\n",
    "#### (d) $ \\frac{\\partial \\log(p(y_i | x_i; w, b))}{\\partial w_j} $\n",
    "\n",
    "The partial derivative of the log likelihood with respect to weight $ w_j $ is:\n",
    "\n",
    "$$\\begin{align} \\frac{d\\log(p(y_i|x_i;w,b))}{dw_j} \n",
    "&= \\frac{d}{dw_j} \\left(y_{i} \\log(p_{i}) + (1 - y_{i}) \\log(1 - p_{i})\\right) \\\\ \n",
    "&= \\frac{d}{dw_j} \\left(y_{i} \\log(\\sigma(w^T x_i + b)) + (1 - y_{i}) \\log(1 - \\sigma(w^T x_i + b))\\right) \\\\ \n",
    "&= y_{i} \\frac{d}{dw_j} \\log(\\sigma(w^T x_i + b)) + (1 - y_{i}) \\frac{d}{dw_j} \\log(1 - \\sigma(w^T x_i + b)) \\\\ \n",
    "&= y_{i} \\frac{1}{\\sigma(w^T x_i + b)} \\cdot \\frac{d}{dw_j} \\sigma(w^T x_i + b) + (1 - y_{i}) \\frac{1}{1 - \\sigma(w^T x_i + b)} \\cdot \\frac{d}{dw_j} (1 - \\sigma(w^T x_i + b)) \\\\ \n",
    "&= y_{i} \\frac{1}{\\sigma(w^T x_i + b)} \\cdot \\sigma(w^T x_i + b) \\cdot (1 - \\sigma(w^T x_i + b)) \\cdot \\frac{d}{dw_j} (w^T x_i + b) + (1 - y_{i}) \\frac{1}{1 - \\sigma(w^T x_i + b)} \\cdot (-\\sigma(w^T x_i + b)) \\cdot (1 - \\sigma(w^T x_i + b)) \\cdot \\frac{d}{dw_j} (w^T x_i + b) \\\\ \n",
    "&= y_{i} \\cdot (1 - \\sigma(w^T x_i + b)) \\cdot x_{ij} + (1 - y_{i}) \\cdot (-\\sigma(w^T x_i + b)) \\cdot x_{ij} \\\\ \n",
    "&= y_{i} \\cdot x_{ij} - y_{i} \\cdot \\sigma(w^T x_i + b) \\cdot x_{ij} - \\sigma(w)^T x_i + b) \\cdot x_{ij} \\\\ \n",
    "&= y_{i} \\cdot x_{ij} - \\sigma(w^T x_i + b) \\cdot x_{ij} \\\\ \n",
    "&= x_{ij} (y_{i} - \\sigma(w^T x_i + b))\\end{align}$$\n",
    "\n",
    "#### (e) $ \\frac{\\partial \\log(p(y_i | x_i; w, b))}{\\partial b} $\n",
    "\n",
    "The partial derivative of the log likelihood with respect to bias $ b $ is:\n",
    "\n",
    "$$\\begin{align} \\frac{d\\log(p(y_i|x_i;w,b))}{db} \n",
    "&= \\frac{d}{db} \\left(y_{i} \\log(p_{i}) + (1 - y_{i}) \\log(1 - p_{i})\\right) \\\\ \n",
    "&= \\frac{d}{db} \\left(y_{i} \\log(\\sigma(w^T x_i + b)) + (1 - y_{i}) \\log(1 - \\sigma(w^T x_i + b))\\right) \\\\ \n",
    "&= y_{i} \\frac{d}{db} \\log(\\sigma(w^T x_i + b)) + (1 - y_{i}) \\frac{d}{db} \\log(1 - \\sigma(w^T x_i + b)) \\\\ \n",
    "&= y_{i} \\frac{1}{\\sigma(w^T x_i + b)} \\cdot \\frac{d}{db} \\sigma(w^T x_i + b) + (1 - y_{i}) \\frac{1}{1 - \\sigma(w^T x_i + b)} \\cdot \\frac{d}{db} (1 - \\sigma(w^T x_i + b)) \\\\ \n",
    "&= y_{i} \\frac{1}{\\sigma(w^T x_i + b)} \\cdot \\sigma(w^T x_i + b) \\cdot (1 - \\sigma(w^T x_i + b)) \\cdot \\frac{d}{db} (w^T x_i + b) + (1 - y_{i}) \\frac{1}{1 - \\sigma(w^T x_i + b)} \\cdot (-\\sigma(w^T x_i + b)) \\cdot (1 - \\sigma(w^T x_i + b)) \\cdot \\frac{d}{db} (w^T x_i + b) \\\\ \n",
    "&= y_{i} \\cdot (1 - \\sigma(w^T x_i + b)) \\cdot 1 + (1 - y_{i}) \\cdot (-\\sigma(w^T x_i + b)) \\cdot 1 \\\\ \n",
    "&= y_{i} - \\sigma(w^T x_i + b) \\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Implementation of the Logistic Regression Classifier\n",
    "\n",
    "The `train` method within the [`LogisticRegressionClassifier`](assignment5/models/classifiers/logistic_regression.py) class is implemented to perform the training of the logistic regression model. This method is responsible for updating the weights and bias of the model through gradient descent, aiming to minimize the cost function:\n",
    "\n",
    "#### (a) Matrix of Covariates \\( X \\)\n",
    "Matrix containing the training data where each row is an observation and each column is a feature. In the context of the `train` function, `X` represents the input data upon which predictions are made.\n",
    "\n",
    "```python\n",
    "n_samples, n_features = X.shape\n",
    "```\n",
    "\n",
    "#### (b) Vector of Labels \\( y \\)\n",
    "The `y` vector holds the actual class labels for each observation in `X`. In binary classification, these labels are either 0 or 1.\n",
    "\n",
    "```python\n",
    "model = self.sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "```\n",
    "\n",
    "#### (c) Initial Weights Vector \\( w \\)\n",
    "The weights vector `w` initially starts with arbitrary values (often zeros) and will be updated through training. It determines the impact of each feature on the decision boundary.\n",
    "\n",
    "```python\n",
    "self.weights = np.zeros(n_features)\n",
    "```\n",
    "\n",
    "#### (d) Initial Bias Value\n",
    "The bias term `b` is analogous to the intercept in linear regression and is used to make adjustments to the decision boundary. It starts at zero and is updated along with the weights.\n",
    "\n",
    "```python\n",
    "self.bias = 0\n",
    "```\n",
    "\n",
    "#### (e) Number of Iterations \\( \\text{num\\_iters} \\)\n",
    "`num_iters` is the number of times the algorithm will work through the entire dataset (each time is an iteration). More iterations can lead to a more accurate model but also take longer to compute.\n",
    "\n",
    "```python\n",
    "for _ in range(num_iters):\n",
    "```\n",
    "\n",
    "#### (f) Learning Rate \\( \\text{learning\\_rate} \\)\n",
    "The `learning_rate` determines how large the steps are during the gradient descent. A smaller learning rate means smaller steps towards the minimum of the cost function.\n",
    "\n",
    "```python\n",
    "self.weights -= learning_rate * dw\n",
    "self.bias -= learning_rate * db\n",
    "```\n",
    "\n",
    "These components work together in the `train` method to iteratively update the weights and bias to minimize the cost function:\n",
    "\n",
    "$$ -\\sum_{i=1}^{N} \\log(p(y_i | x_i; w, b)) $$\n",
    "\n",
    "#### (g) Gradient Descent Updates\n",
    "In each iteration, gradients are calculated for the weights and bias. These gradients indicate the direction in which the cost function has the steepest ascent. By moving in the opposite direction, we aim to find the minimum.\n",
    "\n",
    "```python\n",
    "dw = (1 / n_samples) * np.dot(X.T, (model - y))\n",
    "db = (1 / n_samples) * np.sum(model - y)\n",
    "```\n",
    "\n",
    "#### (h) Parameter Updates\n",
    "After computing the gradients, the weights and bias are updated in the direction that will reduce the cost function.\n",
    "\n",
    "```python\n",
    "self.weights -= learning_rate * dw\n",
    "self.bias -= learning_rate * db\n",
    "```\n",
    "\n",
    "<div class='alert alert-info'> \n",
    "By repeatedly applying these steps for the specified number of iterations, the `train` method effectively tunes the weights and bias to fit the model to the training data, aiming to reduce prediction error and improve model accuracy. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classifier Summary:\n",
      "--------------------------------------------------\n",
      "Weights: [-0.0649, -1.2060, 17.8235]\n",
      "Bias: -0.1075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hectellian/Dev/Autumn2023/13X005-AI/assignment5/assignment5/models/classifiers/logistic_regression.py:10: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(z)) if z < 0 else 1 / (1 + np.exp(-z))\n"
     ]
    }
   ],
   "source": [
    "lr_classifier = LogisticRegressionClassifier()\n",
    "lr_classifier.train(X_train, y_train, num_iters=1000, learning_rate=0.001)\n",
    "\n",
    "print(lr_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will evaluate both the Naive Bayes and Logistic Regression models using the test data. The evaluation metrics will include accuracy, precision, recall, and F1 score. \n",
    "\n",
    "The evaluation of both the Naive Bayes and Logistic Regression models on the test data yields the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.658537</td>\n",
       "      <td>0.782609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.811881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy  Precision    Recall  F1 Score\n",
       "0          Naive Bayes  0.750000   0.964286  0.658537  0.782609\n",
       "1  Logistic Regression  0.683333   0.683333  1.000000  0.811881"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = test_df.drop('Purchased', axis=1).values\n",
    "y_test = test_df['Purchased'].values\n",
    "\n",
    "nb_predictions = nb_classifier.predict(X_test)\n",
    "lr_predictions = lr_classifier.predict(X_test)\n",
    "\n",
    "evaluation_metrics = {\n",
    "    'Model': ['Naive Bayes', 'Logistic Regression'],\n",
    "    'Accuracy': [accuracy_score(y_test, nb_predictions), accuracy_score(y_test, lr_predictions)],\n",
    "    'Precision': [precision_score(y_test, nb_predictions), precision_score(y_test, lr_predictions)],\n",
    "    'Recall': [recall_score(y_test, nb_predictions), recall_score(y_test, lr_predictions)],\n",
    "    'F1 Score': [f1_score(y_test, nb_predictions), f1_score(y_test, lr_predictions)]\n",
    "}\n",
    "\n",
    "\n",
    "evaluation_df = pd.DataFrame(evaluation_metrics)\n",
    "\n",
    "evaluation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Naive Bayes** shows a higher accuracy and precision but lower recall compared to Logistic Regression. This indicates it is more precise but less sensitive in identifying positive cases.\n",
    "- **Logistic Regression** shows a lower accuracy and precision but perfect recall, meaning it identifies all positive cases but at the cost of more false positives.\n",
    "The choice between these models would depend on the specific requirements of the task at hand, whether precision or recall is more valued."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment5-284AL0x7-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
